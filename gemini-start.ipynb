{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = os.getenv(\"LANGSMITH_TRACING\")\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = os.getenv(\"LANGSMITH_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-1.5-pro', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x122164350>, default_metadata=())"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "GOOGLE_API_KEY = os.environ[\"GOOGLE_API_KEY\"]\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=GOOGLE_API_KEY)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Generative AI refers to a category of artificial intelligence algorithms that can create new content, ranging from text and images to audio, code, 3D models, and other data.  Instead of simply analyzing or classifying existing data, generative AI learns the underlying patterns and structure of the input training data and then uses this knowledge to generate similar but novel outputs.\\n\\nHere's a breakdown of key aspects:\\n\\n* **Creation of New Content:**  This is the core principle.  Generative AI doesn't just regurgitate existing data; it generates something new, albeit based on the patterns it learned.\\n* **Learning from Data:** Generative models are trained on large datasets.  The model learns the statistical properties and relationships within the data.\\n* **Variety of Outputs:**  Generative AI can create various content types, including:\\n    * **Text:**  Poems, code, scripts, articles, summaries.\\n    * **Images:**  Photos, artwork, logos, designs.\\n    * **Audio:**  Music, sound effects, voiceovers.\\n    * **Code:**  Software programs, scripts, website layouts.\\n    * **3D Models:**  Objects, environments, characters.\\n    * **Other Data:**  Scientific data, simulations, etc.\\n* **Common Techniques:**  Several techniques are used to build generative AI models:\\n    * **Generative Adversarial Networks (GANs):**  Two neural networks (a generator and a discriminator) compete against each other to improve the quality of the generated output.\\n    * **Variational Autoencoders (VAEs):**  Encode data into a lower-dimensional representation and then decode it back to generate new samples.\\n    * **Transformer-based models:**  Utilize attention mechanisms to process sequential data, commonly used for text generation (e.g., GPT-3, LaMDA).\\n    * **Diffusion models:**  Gradually add noise to training data and then learn to reverse this process to generate data from noise.\\n\\n* **Applications:** Generative AI has a wide range of applications:\\n    * **Art and Creativity:**  Generating art, music, and other creative content.\\n    * **Content Creation:**  Writing articles, scripts, and marketing materials.\\n    * **Software Development:**  Generating code and automating testing.\\n    * **Drug Discovery:**  Designing new molecules and predicting their properties.\\n    * **Design and Engineering:**  Creating 3D models and optimizing designs.\\n    * **Personalized Experiences:**  Tailoring content and recommendations to individual users.\\n\\n\\nWhile generative AI offers exciting possibilities, it's important to be aware of potential ethical considerations, such as the creation of deepfakes, copyright issues, and the potential for misuse.  The field is constantly evolving, with ongoing research focused on improving the quality, controllability, and ethical implications of generated content.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-ac2ce8c6-7034-4e26-960d-7f9d5150e9d7-0' usage_metadata={'input_tokens': 6, 'output_tokens': 597, 'total_tokens': 603, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"What is generative AI?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI Engineer. Provide me answers based on the questions\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"LangSmith is a platform specifically designed to improve and monitor the performance of large language models (LLMs).  It offers a suite of tools that cater to the entire LLM lifecycle, from debugging and evaluation to monitoring and continuous improvement.  Here's a breakdown of its key features and benefits:\\n\\n**Key Features:**\\n\\n* **Tracing and Debugging:** LangSmith captures detailed traces of LLM interactions, including prompts, completions, intermediate steps (like calls to tools or chains), and metadata. This allows developers to understand exactly how their LLM is behaving and identify the root cause of errors or unexpected outputs.  Think of it as a debugger specifically for LLMs.\\n\\n* **Evaluation and Benchmarking:**  The platform facilitates robust evaluation of LLM performance across different datasets and metrics.  You can define custom metrics relevant to your specific application and track performance over time. This helps in comparing different LLM versions, prompt engineering strategies, or even different LLMs altogether.\\n\\n* **Monitoring and Alerting:** LangSmith provides real-time monitoring of LLM performance in production environments.  You can set up alerts for critical issues like performance regressions, high latency, or unexpected outputs, enabling proactive intervention and ensuring a smooth user experience.\\n\\n* **Data Management and Versioning:**  It helps manage training data, evaluation datasets, and different versions of your prompts and LLMs. This enables systematic experimentation and facilitates rollback to previous versions if needed.\\n\\n* **Collaboration and Teamwork:** LangSmith is designed for collaborative workflows, allowing teams to share traces, evaluations, and insights.  This fosters knowledge sharing and accelerates the debugging and improvement process.\\n\\n\\n**Benefits:**\\n\\n* **Improved LLM Performance:** By providing in-depth insights into LLM behavior, LangSmith enables targeted improvements in prompt engineering, model selection, and overall system design.\\n\\n* **Faster Debugging:**  The tracing and debugging capabilities significantly reduce the time it takes to identify and fix issues in LLM applications.\\n\\n* **Reduced Risk in Production:** Real-time monitoring and alerting help prevent and mitigate issues that could negatively impact user experience.\\n\\n* **Data-Driven Optimization:** The platform enables data-driven decision-making by providing quantitative metrics and insights into LLM performance.\\n\\n* **Streamlined Workflow:**  LangSmith integrates with popular LLM frameworks and tools, simplifying the development and deployment process.\\n\\n\\n**Who should use LangSmith?**\\n\\n* **LLM Developers:**  Those building and deploying LLM-powered applications will find LangSmith invaluable for debugging, evaluation, and monitoring.\\n\\n* **Machine Learning Engineers:**  ML engineers working with LLMs can leverage LangSmith to track experiments, compare models, and optimize performance.\\n\\n* **Data Scientists:**  Data scientists can use the platform to analyze LLM behavior, identify biases, and improve the quality of training data.\\n\\n* **Product Managers:**  Product managers can gain insights into LLM performance and user experience, informing product strategy and roadmap.\\n\\n\\nIn summary, LangSmith is a powerful platform that empowers developers and teams to build, debug, and optimize LLM applications effectively. It addresses the unique challenges of working with LLMs and provides the tools needed to ensure reliable and high-performing LLM-powered systems.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-0f178da9-7da6-47c9-8667-fe31ba0208d9-0', usage_metadata={'input_tokens': 23, 'output_tokens': 659, 'total_tokens': 682, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt|llm\n",
    "response = chain.invoke({\"input\": \"Can you tell me about Langsmith?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LangSmith is a platform specifically designed to improve and monitor the performance of large language models (LLMs).  It goes beyond simple evaluation metrics and provides tools to deeply understand LLM behavior, debug issues, and iteratively refine prompts and models.  Here's a breakdown of its key features and benefits:\\n\\n**Key Features:**\\n\\n* **Tracing & Debugging:** LangSmith captures a detailed execution trace of each LLM call, including prompts, responses, intermediate steps, and metadata. This allows developers to pinpoint the root cause of errors, understand the flow of reasoning, and identify areas for improvement.  Think of it as a debugger specifically for LLM workflows.\\n* **Evaluation & Monitoring:**  It provides tools to evaluate LLM performance across various metrics like accuracy, relevance, and toxicity.  It also allows for ongoing monitoring of these metrics to detect regressions and track progress over time.  You can define custom metrics relevant to your specific application.\\n* **Dataset Management & Versioning:** LangSmith helps manage and version training and evaluation datasets, making it easier to track experiments and reproduce results.  This is crucial for iterative development and maintaining consistent benchmarks.\\n* **Experiment Tracking:**  The platform facilitates organized experimentation by tracking different prompt variations, model parameters, and datasets.  This simplifies comparison and analysis of different approaches.\\n* **Collaboration & Sharing:** LangSmith promotes collaboration among team members by providing a shared workspace for projects, datasets, and evaluations.\\n* **Prompt Engineering & Optimization:**  Through detailed analysis of LLM behavior, LangSmith assists in refining prompts to elicit desired responses.  This includes identifying problematic prompt patterns and suggesting improvements.\\n* **Integration with Popular LLM Providers:**  It integrates with leading LLM providers like OpenAI, Cohere, Hugging Face, and others, simplifying the process of connecting and managing different models.\\n* **Customizable & Extensible:** The platform allows for customization and extension through APIs and integrations, enabling developers to tailor it to their specific needs.\\n\\n**Benefits of Using LangSmith:**\\n\\n* **Improved LLM Performance:** By providing deep insights into LLM behavior, LangSmith helps developers identify and address performance bottlenecks.\\n* **Faster Debugging & Iteration:** The tracing and debugging capabilities significantly reduce the time spent on troubleshooting LLM issues.\\n* **Enhanced Collaboration:** The shared workspace and collaboration features streamline teamwork and knowledge sharing.\\n* **Data-Driven Optimization:**  The platform's evaluation and monitoring tools enable data-driven decisions for optimizing prompts, models, and datasets.\\n* **Reduced Development Costs:**  By accelerating the development process and improving LLM performance, LangSmith can contribute to lower overall project costs.\\n\\n\\n**Who should use LangSmith?**\\n\\n* **Machine Learning Engineers:**  Those building and deploying LLM-powered applications.\\n* **Prompt Engineers:**  Those focused on crafting and optimizing prompts for specific tasks.\\n* **Data Scientists:**  Those analyzing LLM performance and iterating on datasets.\\n* **AI Researchers:**  Those exploring and experimenting with new LLM techniques and applications.\\n\\nIn summary, LangSmith is a powerful platform that empowers developers to build, debug, and optimize LLM applications more effectively. Its comprehensive features and focus on observability make it a valuable tool for anyone working with large language models.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## stroutput parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt|llm|output_parser\n",
    "response = chain.invoke({\"input\": \"Can you tell me about Langsmith?\"})\n",
    "response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
